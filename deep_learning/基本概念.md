# 机器学习4个分支
* 监督学习——已知目标
    1. 序列生成（sequence generation)
    2. 语法书预测（syntax tree prediction)
    3. 目标检测(object detection)
    4. 图像分割（image segmentation)
* 无监督学习 —— 不知道目标
    1. 降维（dimensionality reduction)
    2. 聚类(clustering)
* 强化学习 —— 接受环境信息，选择使某种奖励最大化的行动
* 自监督学习 —— 从输入数据中生成目标
    1.自编码器（autoencoder)

# 评估机器学习模型
## 训练集，验证集，测试集合
    在训练集上训练，在验证集上评估模型，得到了最佳参数，再在测试数据上最后测试一次。 
## 数据少时可以采用留出验证，K折验证，打乱数据的重复K折验证
### 留出验证（hold-out validation)

```
num_validation_samples = 10000
np.random.shuffle(data) #打乱数据
validation_data = data[:num_validation_samples] # 定义验证集
data = data[num_validation_samples:]
training_data = data[:] # 定义训练集

# 在训练集上训练模型，在验证集上评估模型
model = get_model() 
model.train(training_data )
validation_score = model.evaluate(valiation_data) 
# 反复循环

# 调节好超参数后，在所有非测试数据上训练最终模型
model = get_model()
model.train(np.concatenate( [training_data,
                             validation_data]))
test_score = model.evaluate(test_data) 
```
缺点： 如果可用的数据很少，那么可能验证集和测试集包含的样本就太少

### K折验证（K-fold validation)
将数据划分为大小相同的K 个分区。对于每个分区 i，在剩 余的K-1 个分区上训练模型，然后在分区 i 上评估模型。最终分数等于K 个分数的平均值。

```
k = 4 num_validation_samples = len(data) // k 
 
np.random.shuffle(data) 
 
validation_scores = [] 
for fold in range(k): 
    validation_data = data[num_validation_samples * fold:num_validation_samples * (fold + 1)] #选择验证数据分区 
    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1):]  # 使用剩余数据作为训练数据， + 是列表合并  
 
    model = get_model()  # 创建一个全新的模型
    model.train(training_data) 
    validation_score =model.evaluate(validation_data)             validation_scores.append(validation_score) 
 
validation_score = np.average(validation_scores)  # 最终得分
 
model = get_model() 
model.train(data) 
test_score = model.evaluate(test_data)
```

### 带有打乱数据的重复K折验证

# 数据预处理，特征工程和特征学习
## 神经网络数据预处理
    1.向量化： 输出和目标都是浮点数张量
    2.值标准化： 转换为0~1范围的值（ `x -= x.mean(axis=0) x /= x.std(axis=0)`)
## 处理缺失值
    1. 缺失值设为0
    2. 对于测试数据集有缺失值的，人为生成缺失项的训练样本
## 特征工程 —— 找到对结果最大影响的特征

# 过拟合和欠拟合
机器学习的根本问题是优化和泛化之间的对立。优化（optimization）是指调节模型以在训 练数据上得到最佳性能（即机器学习中的学习），而 泛化（generalization）是指训练好的模型在 前所未见的数据上的性能好坏。
## 减少过拟合
* 获取更多的训练数据
* 正则化（regularization)    
    1.减少网络大小    
    2.添加权重正则化    
        >奥卡姆剃刀（Occam’s razor）原理：如果一件事情有两种解释，那么最可能正 确的解释就是最简单的那个，即假设更少的那个 



        权重正则化: 强制让模型权重只能取参数值分布的熵较小的值， 从而限制模型的复杂度，2种形式：
            * L1正则化： 添加的成本与权重系数的绝对值［权重的 L1 范数（norm）］ 成正比
            * L2正则化： 添加的成本与权重系数的平方（权重的 L2 范数）成正比。  也叫权重衰减（weight decay)    
    
    3.添加dropout正则化： 在训练过程中随机将该层的一些输出特征舍 弃（设置为0）
    dropout 比率（dropout rate）是被设为0 的特征所占的比例，通常在0.2~0.5 范围内
